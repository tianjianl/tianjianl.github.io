<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://tianjianl.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tianjianl.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-02-14T03:41:27+00:00</updated><id>https://tianjianl.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">In Defense of Vanilla DPO for Language Model Alignment</title><link href="https://tianjianl.github.io/blog/2024/dpo/" rel="alternate" type="text/html" title="In Defense of Vanilla DPO for Language Model Alignment" /><published>2024-12-06T15:12:00+00:00</published><updated>2024-12-06T15:12:00+00:00</updated><id>https://tianjianl.github.io/blog/2024/dpo</id><content type="html" xml:base="https://tianjianl.github.io/blog/2024/dpo/"><![CDATA[<p>Direct Preference Optimization (DPO; <a href="https://arxiv.org/abs/2305.18290">Rafailov et al., 2023</a>) has emerged as the de-facto alignment algorithm for language models. It has been employed by many industrial level language models, which includes but is not limited to Meta’s Llama 3 (<a href="https://arxiv.org/abs/2407.21783">Dubey et al., 2024</a>), AI2’s Tulu (<a href="https://arxiv.org/abs/2411.1512">Lambert et al., 2024</a>), Alibaba’s Qwen (<a href="https://arxiv.org/abs/2407.10671">Yang et al., 2024</a>). The intuition is that DPO aims to <strong>directly</strong> increase the log-likelihood of human-preferred responses while decreasing the log-likelihood of dispreferred ones. Compared to standard RLHF (<a href="https://arxiv.org/abs/2009.01325">Stiennon et al., 2020</a>), the asymptotically equivalent DPO shines due to its simplicity - It requires neither on-policy sampling nor training a separate reward model.</p>

<p>Despite its simplicity and effectiveness, DPO has faced many criticisms (<a href="https://arxiv.org/abs/2405.19534">Chen et al., 2024</a>, <a href="https://arxiv.org/abs/2402.13228">Pal et al., 2024</a>,<a href="https://arxiv.org/abs/2404.12358">Rafailov et al., 2024</a>,<a href="https://arxiv.org/abs/2410.08847v2">Razin et al., 2024</a>, <a href="https://arxiv.org/abs/2405.14734">Meng et al., 2024</a>, <a href="https://kyunghyuncho.me/a-proper-preference-optimization-loss-and-its-gradient/">Cho 2024</a>). We summarize the criticisms below:</p>

<ul>
  <li>
    <p><strong>Likelihood Displacement</strong>: The likelihood of the chosen and the rejected response simultaneously decreases. Existing work accounts this to either the edit distance between the chosen and the rejected is short (<a href="https://arxiv.org/abs/2402.13228">Pal et al., 2024</a>) or the embedding distance the chosen and rejected is small (<a href="https://arxiv.org/abs/2410.08847v2">Razin et al., 2024</a>). Too similar pairs make the model fail to differentiate between the chosen and the rejected, resulting in the model simutaneously reducing the likelihood of both.</p>
  </li>
  <li>
    <p><strong>Poor Ranking Accuracy</strong>: Even after DPO, the model fails to assign higher probability to chosen responses compared to rejected responses (<a href="https://arxiv.org/abs/2405.19534">Chen et al., 2024</a>). This is an expected outcome due to likelihood displacement.</p>
  </li>
  <li>
    <p><strong>Mismatch Between Data and Policy</strong>: During pure offline DPO, where the preference data are collected from various models, e.g., Ultrafeedback (<a href="https://arxiv.org/abs/2310.01377">Cui et al., 2024</a>), there exists a mismatch between what the model can generate and what is optimized. To mitigate this issue, there has been many works that propose on-policy DPO (<a href="https://arxiv.org/abs/2312.16682">Xu et al., 2024</a>, <a href="https://arxiv.org/abs/2309.00267v3">Guo et al., 2024</a>, <a href="https://arxiv.org/abs/2401.10020">Yuan et al., 2024</a>) that generates the pair of responses from the reference policy rather than collecting preferences offline. Another solution would be to train the model first on chosen responses (<a href="https://arxiv.org/abs/2405.14734">Meng et al., 2024</a>).</p>
  </li>
</ul>

<p>A natural question arises: <strong>despite all these criticisms, why is vanilla DPO still the go-to algorithm when aligning language models?</strong></p>

<p>In this blogpost, I aim to give a non-rigorous explanation of the following two research questions:</p>

<ul>
  <li>Why does the likelihood displacement phenomenon occur?</li>
  <li>Why DPO works despite likelihood displacement on off-policy data?</li>
</ul>

<p>By answering these questions, I aim to defend against vanilla DPO for alignment, and to shed light on a deeper understanding of what DPO is doing to our model.</p>
<h3 id="preliminaries">Preliminaries:</h3>

<p>For a given user prompt \(x\), the language model \(\pi(\cdot \mid x)\) yields a probablity distribution over all possible responses \(\mathbf{y}\).</p>

<p>The pipeline begins with training the base language model \(\pi_\text{base}\) on high-quality generations \(\mathcal{D}_\text{SFT} = \{(x, \mathbf{y}), ...\}\). The SFT loss is given by</p>

\[\mathcal{L}_\textrm{SFT} = - \log \pi(\mathbf{y} \mid x)\]

<p>This stage of the pipeline is called Supervised Fine-Tuning (SFT), which produces the SFT’ed model \(\pi_\textrm{SFT}\).</p>

<p>Then there is another dataset \(\mathcal{D}_\textrm{DPO} = \{(x, \mathbf{y}_w, \mathbf{y}_l) ... \}\), where there are two generations \((y_w, y_l)\) for a given prompt \(x\), where \(\mathbf{y}_w\) is preferred (usually referred to as the “winning” or “chosen” response), and \(\mathbf{y}_l\) is dispreferred (usually referred to as the “losing” or “rejected” response). 
The DPO loss is given by:</p>

\[\mathcal{L}_\textrm{DPO} = -\log \sigma\left(\beta\log \frac{\pi_\theta(\mathbf{y}_w \mid x)}{\pi_\text{SFT}(\mathbf{y}_w \mid x)} - \beta\log\frac{\pi_{\theta}(\mathbf{y}_l \mid x)}{\pi_\textrm{SFT}(\mathbf{y}_l \mid x)}\right),\]

<p>which aims to maximize the margin between the increase in log-probability of the chosen response \(\log \pi_\theta(\mathbf{y}_w \mid x) - \log \pi_\text{SFT}(\mathbf{y}_w \mid x)\), and the increase in log-probability of the rejected response \(\log \pi_\theta(\mathbf{y}_l \mid x) - \log \pi_\text{SFT}(\mathbf{y}_l \mid x)\). In an ideal setting, the DPO loss should push the log-likelihood of the chosen response higher and push the log-likelihood of the rejected response lower, making the preferred response more likely under the aligned model.</p>

<p>However, as many works have noticed (<a href="https://arxiv.org/abs/2404.12358">Rafailov et al., 2024</a>, <a href="https://arxiv.org/abs/2402.13228">Pal et al., 2024</a>, <a href="https://arxiv.org/abs/2410.08847v2">Razin et al., 2024</a>, <a href="https://arxiv.org/abs/2404.04626">Feng et al., 2024</a>), often times the <strong>likelihood of both the chosen and the rejected response goes down</strong>, a phenomenon that <a href="https://arxiv.org/abs/2410.08847v2">Razin et al., 2024</a> terms “likelihood displacement”. In this blogpost, I would like to offer another explanation of why does “likelihood displacement” happens and more interestingly, why DPO is a strong baselines despite “likelihood displacement”. To this, we first need to investigate the effect of DPO onto the language modeling distribution — “The Squeezing Effect”.</p>

<h2 id="the-squeezing-effect-of-dpo-negative-gradient">The Squeezing Effect of DPO Negative Gradient</h2>

<p>The DPO gradient consists of two parts, a positive gradient that increases the likelihood of the chosen response, and a negative gradient that decreases the likelihood of the rejected response. <a href="https://arxiv.org/abs/2404.04626">Feng et al., 2024</a> shows that the negative gradient trumps the positive one.</p>

<p>If we take one step further and dig into the negative gradient, <a href="https://arxiv.org/abs/2407.10490">Ren and Sutherland, 2024</a> introduced an interesting effect of the negative gradient in DPO: both the chosen and rejected response goes down, and as a result, <strong>the majority of the probability mass is carried to sequences with very high likelihood.</strong> Intuitively, the “rich gets richer”, i.e. sentences with already high probability gets even higher due to the DPO negative gradient.</p>

<p>In the paper, they wrote as:</p>

<p>”The decreased probability mass is largely “squeezed” into the output which was most confident before the update. That is, if \(y^* = \arg \max_{i \in [V]\backslash \{y_l\}} \pi_{\theta}^t(y = i)\) , then \(\pi_{y = y^*}\) is guaranteed to increase.“</p>

<p>The following figure from <a href="https://arxiv.org/abs/2407.10490">Ren and Sutherland, 2024</a> illustrates the squeezing effect of DPO. As the log-likelihood of the rejected sentence \(\pi(\mathbf{y}_{u}^-)\) gets pushed down, the majority of the increased mass goes to \(\mathbf{y^*}\), the most probable sentence, instead of the chosen (winning) sentence \(\mathbf{y}_u^{+}\).</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Squeezing-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Squeezing-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Squeezing-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Squeezing.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="400" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>To validate this claim, <a href="https://arxiv.org/abs/2407.10490">Ren and Sutherland, 2024</a> performed a series of experiments: they plot out the log-likelihood of the chosen sentence and its paraphrases, the rejected sentence and its paraphrases, and a proxy of \(\mathbf{y}^*\) - the greedy decoded sentence: the following figure from the paper illustrates this:</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/chosen_rejected_decrease-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/chosen_rejected_decrease-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/chosen_rejected_decrease-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/chosen_rejected_decrease.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>In the left 4 subfigures, we observe that the likelihood of the chosen and its paraphrases, the rejected and its paraphrases all get decreased during DPO. However, the greedy decoded sentence, which is the black one in the rightmost subfigure goes up, thus confirming that the most the the probability mass goes to \(\mathbf{y}^*\)(or at least a proxy of \(\mathbf{y}^*\)).</p>

<p>Therefore, we can give a explanation of why “likelihood displacement” happens: it is due to the DPO negative gradient squeezes the probability masses to \(\mathbf{y}^*\) , often leading to decrease of both  \(\pi(\mathbf{y}_w \mid x)\) and \(\pi(\mathbf{y}_l \mid x)\). Now we aim to answer the second question: why are the benefits of increasing \(\mathbf{y}^*\)?</p>
<h2 id="the-benefits-of-increasing-mathbfy">The Benefits of Increasing \(\mathbf{y}^*\)</h2>

<p>Now that we know the reason why DPO decreases both the chosen and the rejected response is because it carried most of the decreased probability mass to \(\mathbf{y}^*\), we want to know why is DPO ,even the naive offline version, is still able to yield strong baselines, and is often the go-to method for industry labs to build strong models.</p>

<p><strong>It turns out that \(\mathbf{y}^* = \arg\max_\mathbf{y} \pi(\mathbf{y} \mid x)\) itself is a very strong baseline.</strong> In <a href="https://arxiv.org/pdf/2412.01951">Huang et al., 2024</a>, the authors were interested in the mechanisms behind self-improvement - why can language models be improved when training on data that is generated by itself (<a href="https://aclanthology.org/P95-1026.pdf">Yarowsky, 1995</a>, <a href="https://arxiv.org/abs/1909.13788">He et al., 2020</a>, <a href="https://arxiv.org/abs/2203.14465">Zelikman et al., 2022</a>, <a href="https://arxiv.org/abs/2312.16682">Xu et al., 2023</a>), sometimes even without external feedback (<a href="https://arxiv.org/abs/2401.10020">Yuan et al., 2024</a>)?</p>

<p><a href="https://arxiv.org/abs/2412.01951">Huang et al., 2024</a> points out that the best-of-N response with the highest likelihood:
\(\mathbf{\hat{y}}^* = \arg\max_{\mathbf{y}_i,~i \in \{1,...,N\}} \pi(y_i\mid x)\)
is actually a strong baseline, and training a language model on high-likelihood responses can “sharpen” the model towards these high-likelihood responses. (It would be interesting to track the best-of-N log-likelihood throughout DPO).</p>

<p>Empirically, <a href="https://arxiv.org/abs/2412.01951">Huang et al., 2024</a> validated their results on a large suite of models (Phi, Mistral, Llama, GPT-3.5) across various tasks (MATH, GSM, …). The results are in the following figure:</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Experiments-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Experiments-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Experiments-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Experiments.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>In subfigure (a), the numbers refer to the percent accuracy improvement over greedy decoding. The authors find that:</p>
<ol>
  <li>Best-of-N (with highest log-likelihood) is always better than naive sampling with temperature = 1.0</li>
  <li>For all datasets, Best-of-N improves upon greedy decoding, for at least one model.</li>
  <li>For every model, there is at least one dataset where Best-of-N improves upon greedy decoding.</li>
</ol>

<p>In the middle subfigure (b), x-axis is the N in best-of-N and the y-axis is the percentage improvement over greedy decoding - this shows that finding sequences with high likelihood is able to yield a much stronger baseline than greedy decoding (which is already a strong baseline).</p>

<p>In the right subfigure (c), the histograms of correct and incorrect responses are plotted, we can see that the correct response tends to have higher likelihood under the base model.</p>

<p>These findings indicate that the “hidden knowledge” for self-improvement is hidden at the probabilities -  sequences with high log-probability is more likely to be correct. \(\mathbf{y}^*\) , or the most probable sentence is most likely to be correct.</p>

<p>So far, we have learned that:</p>

<ul>
  <li>
    <p>DPO pushes up sentences with <strong>super high log-probability</strong> under the model, especially when the winning and losing response are offline samples (<a href="https://arxiv.org/abs/2407.10490">Ren and Sutherland, 2024</a>).</p>
  </li>
  <li>
    <p>Sentences with <strong>super high log-probability</strong> are more likely to be correct answers / better responses (<a href="https://arxiv.org/abs/2412.01951">Huang et al., 2024</a>).</p>
  </li>
</ul>

<p>Now, It should be intuitive that why DPO (even the offline variant) is able to yield a strong baseline - it “squeezes” or “sharpens” up the log-likelihood of sentences with already large log-probability, regardless of the winning and losing response construction (because it is very unlikely that the winning and / or the losing response is \(\mathbf{y}^*\)). There is also empirical evidence that performing vanilla DPO on “wrong-over-wrong” responses (<a href="https://arxiv.org/abs/2410.11055">Yao et al., 2024</a>) can improve performance.</p>

<h2 id="limitations">Limitations</h2>

<p>Admittedly, this explanation of DPO moves masses to \(\mathbf{y}^*\) is a simplified version of what is happening in practice, otherwise we would be living in a world where we simple impose the negative gradient on every offline data to sharpen the model towards high log-prob sentences. But I think this explanation is interesting and motivates to think of more reliable reward signals beyond model log-probs.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="alignment" /><summary type="html"><![CDATA[Direct Preference Optimization (DPO; Rafailov et al., 2023) has emerged as the de-facto alignment algorithm for language models. It has been employed by many industrial level language models, which includes but is not limited to Meta’s Llama 3 (Dubey et al., 2024), AI2’s Tulu (Lambert et al., 2024), Alibaba’s Qwen (Yang et al., 2024). The intuition is that DPO aims to directly increase the log-likelihood of human-preferred responses while decreasing the log-likelihood of dispreferred ones. Compared to standard RLHF (Stiennon et al., 2020), the asymptotically equivalent DPO shines due to its simplicity - It requires neither on-policy sampling nor training a separate reward model.]]></summary></entry><entry><title type="html">Manipulating Gradients to Balance Multilingual Machine Translation Models</title><link href="https://tianjianl.github.io/blog/2023/mnmt/" rel="alternate" type="text/html" title="Manipulating Gradients to Balance Multilingual Machine Translation Models" /><published>2023-11-08T15:12:00+00:00</published><updated>2023-11-08T15:12:00+00:00</updated><id>https://tianjianl.github.io/blog/2023/mnmt</id><content type="html" xml:base="https://tianjianl.github.io/blog/2023/mnmt/"><![CDATA[<h2 id="overview">Overview</h2>

<p>Finding out the reasons and solutions for negative interference in Multilingual Neural Machine Translation [<a href="https://arxiv.org/abs/1611.04558">Johnson et al., 2016</a>; <a href="https://aclanthology.org/N19-1388">Aharoni et al., 2019</a>] has been an active research area for the past 5-7 years. Yet, while previous studies [<a href="https://arxiv.org/abs/2010.05874">Wang et al., 2020</a>] find that negative interference mainly occurs between different language families, recent studies [<a href="https://arxiv.org/abs/2212.07530">Shaham et al., 2023</a>] have demonstrated that negative inference does not happen between languages of different families. The interference emerges because of the mismatch in the amount of data for different translation directions. Real-world translation data suffers from a heavy mismatch of data in different directions, ranging from less than 100K to over 100M [<a href="https://arxiv.org/abs/2207.04672">NLLB Team, 2022</a>], so it is crucial to find balancing methods that are both scalable and robust.</p>

<p>This blog post aims to give an overview of the two approaches for handling imbalances in multilingual neural machine translation: <strong>Scalarization</strong> and <strong>Gradient projection</strong>. Both methods have pros and cons, and there still needs to be a consensus on which method performs the best. This blog post will cover some of the most up-to-date methods for handling data size mismatches and interference.</p>

<p><strong>Disclaimer:</strong> The papers introduced in this blog post are only representative works rather than a comprehensive survey to give an overview of the different methods to handle data imbalance in translation directions.</p>

<h2 id="background">Background</h2>

<h4 id="multilingual-neural-machine-translation">Multilingual Neural Machine Translation</h4>

<p>We start by describing some basics in multilingual neural machine translation: we are interested in mapping a source sequence \(\textbf{x}_s = \{x_1, x_2, ..., x_n\}\) in language \(s\) to a target sequence \(\textbf{y}_t = \{y_1, y_2, ..., y_m\}\) in language \(t\). We train an autoregressive model parameterized by \(\theta\) that predicts each target token conditioning on the entire source sentence and the target tokens before it:</p>

\[\mathcal{L}_{s,t}(\theta) = \sum_{i=1}^m \log p_\theta(y_i \mid \mathbf{y}_{&lt;i}, \mathbf{x}).\]

<p>In multilingual neural machine translation, multiple source-target pairs are concatenated to form a large dataset. Given parallel sentences in \(N\) languages pairs \((s_1, t_1), (s_2, t_2),... (s_N, t_N)\), a naive multilingual machine translation model aims to minimize an unweighted sum of the losses of individual translation directions:</p>

\[\mathcal{L}_\text{MMT}(\theta) = \sum_{i=1}^N \mathcal{L}_{s_i, t_i}(\theta)\]

<p>Here, the parameters for each translation direction are shared, which is much more compute-efficient than training individual models for each direction. Different directions might benefit from each other, resulting in positive transfer.</p>

<h4 id="pareto-front">Pareto Front</h4>

<p>Multilingual Translation can be seen as a multi-task learning (or multi-objective optimization) problem [<a href="https://aclanthology.org/N19-1388/">Ahroni et al., 2019</a>], where each translation direction is an individual task. We are interested in finding the optimal solutions in that we cannot improve the performance of an individual task without sacrificing the performance of other tasks. Such solutions are called <strong>Pareto optimal solutions</strong> [<a href="https://web.stanford.edu/~boyd/cvxbook/">Boyd and Vandenberghe, 2004</a>]. The set of all Pareto optimal solutions forms the <strong>Pareto Front</strong> of a given multi-objective optimization problem. See Figure 1 for an illustration of the Pareto front (figure pasted from <a href="https://arxiv.org/abs/2302.09650">Fernandes et al., 2023</a>).</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Screenshot%202023-11-08%20at%2011.43.59%E2%80%AFPM-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Screenshot%202023-11-08%20at%2011.43.59%E2%80%AFPM-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Screenshot%202023-11-08%20at%2011.43.59%E2%80%AFPM-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Screenshot%202023-11-08%20at%2011.43.59%E2%80%AFPM.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="391" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h2 id="scalarization">Scalarization</h2>

<p>The heavy mismatch in data sizes causes the naive unweighted average of individual losses to amplify the performance of high-resource languages (HRLs) and decrease the performance of low-resource languages (LRLs). In practice, we sample a batch of input and output sentences. Since data of HRLs can be more than 10x of data of LRLs, we are far more likely to sample from HRLs, resulting in the optimization process heavily favoring optimizing towards better performance on HRLs.</p>

<p>To mitigate this, instead of using proportional sampling, we can use temperature sampling [<a href="https://arxiv.org/abs/1907.05019">Arivazhagan et al., 2019</a>]. The probability of sampling from each direction is given by:</p>

\[p_{(s_i, t_i)} = \frac{D(s_i, t_i)^\frac{1}{\tau}}{\sum_{j=1}^N D(s_j, t_j)^\frac{1}{\tau}}\]

<p>Where \(D(s_i, t_i)\) is the datasize of translation direction \(s_i \rightarrow t_i\) , and \(\tau\) is the sampling temperature. When \(\tau = 1\), our sampling method is equivalent to naive proportional sampling. As \(\tau\)  gets larger, we are decreasing the weights on HRLs and increasing the weights on LRLs. As \(\tau \rightarrow +\infty\), the sampling strategy becomes a uniform distribution over all language pairs.</p>

<p>A common understanding in the machine translation literature is that tuning the temperature \(\tau\) is equivalent to tuning the weights for each translation direction in a weighted sum of individual losses, which we refer to as <strong>scalarization</strong>.</p>

\[\mathcal{L}_\text{MMT}(\theta) = \sum_{i=1}^N w_i \mathcal{L}_{s_i, t_i}(\theta)\]

<p>Although the equivalency of tuning the temperature \(\tau\) and tuning the weights \(w_i\) have not been thoroughly established, in this blog post, I will use <strong>sampling ratio</strong> and <strong>task weights</strong> interchangeably and introduce some of the latest advances in how to find fantastic weights that achieves good performance.</p>

<h4 id="static-weights">Static Weights</h4>

<p><a href="https://arxiv.org/abs/2302.09650">Fernandes et al., 2023</a> show we can find the Pareto front for multilingual translation by varying the sampling ratio. However, their work assumes that we have an even amount of data for each translation direction, which is often untrue in real-world settings. A follow-up work [<a href="https://arxiv.org/abs/2304.03216">Chen et al., 2023</a>] shows that when there is a data size mismatch, the Pareto front collapses: see the following figure from the paper for a comparison between the Pareto curve when data is balanced VS data is imbalanced.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Screenshot%202023-11-08%20at%204.45.32%E2%80%AFPM-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Screenshot%202023-11-08%20at%204.45.32%E2%80%AFPM-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Screenshot%202023-11-08%20at%204.45.32%E2%80%AFPM-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Screenshot%202023-11-08%20at%204.45.32%E2%80%AFPM.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="602" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p><a href="https://arxiv.org/abs/2304.03216">Chen et al., 2023</a> presents a comprehensive study on how to tune the sampling temperature (or, equivalently, task weights) for each translation direction. Given a fixed sampling ratio \(p\) and the number of training examples \(D\) for a given direction, the cross-entropy loss can be expressed as:</p>

\[\mathcal{L}(p, D) = (k\cdot p)^{-\alpha} + (D^\gamma + b) \cdot(q\cdot p)^{\beta}+M_{\infty}\]

<p>The given parameters are the sampling ratio \(p\), the data size \(D\), and a constant bias term \(M_{\infty}\), and the rest of the parameters are to be estimated. To estimate these parameters, the authors conducted a series of experiments on WMT \(\text{English} \rightarrow \{\text{French, German}\}\) data and varied the amount of available data \(D\) and sampling ratio \(p\) to form a curve and estimate parameters that best fit this curve. Experiments show that their estimated scaling laws also generalize to other language pairs.</p>

<p>So the question remains: how to find the best \(p\) for each direction? The author frames this as an optimization problem: given a fixed number of \(D\) and a pre-defined importance of each translation direction \(r\) for each task, the optimization problem is:</p>

\[\mathbf{p} = \arg\min_p \mathcal{L}(p;r;d)\]

\[\textrm{subject to}~\mathcal{L(\mathbf{p};r;d}) = \sum_{i}r_i\mathcal{L}(p_i, D_i)\]

\[\mathbf{p}&gt;0\]

\[\sum_{i} p_i =1,~~~\sum_{i} r_i =1\]

<p>The standard way is to assume each translation direction has equal importance \(r_i = \frac{1}{N}\), but this is customizable if you want to emphasize some translation directions. Such a static weighting method for multilingual translation produces strong baselines over using a fixed temperature (e.g., \(\tau = 1, 5, 100\)) and fancy gradient projection techniques (which we will introduce later).</p>

<h4 id="dynamic-weights">Dynamic Weights</h4>

<p>But why do we need static weights for each direction? The mismatch in data sizes causes a mismatch in convergence rates, as <a href="https://arxiv.org/abs/2205.01620">Huang et al., 2022</a> points out, while LRLs have already converged, the HRLs have not yet converged, and continuing training on all tasks results in overfitting on the LRLs. This motivates methods that tackle the imbalanced problem with dynamic sampling temperature methods that consider that different directions converge at different rates.</p>

<p>A naive way is to focus on one or a set of translation directions that converges the slowest during training: In statistical learning, Distributionally Robust Optimization methods [<a href="https://arxiv.org/abs/1909.02060">Oren et al., 2019</a>; <a href="https://openreview.net/forum?id=ryxGuJrFvS">Sagawa et al., 2020</a>; <a href="https://arxiv.org/abs/2109.04020">Zhou et al., 2021</a>], instead of minimizing the sum of all losses, tries to minimize the loss of the worst performing group, forming a min-max optimization problem:</p>

\[\min_\theta \max_{s, t} \mathcal{L_{s ,t}(\theta)}\]

<p>However, naively minimizing the worse-performing language pair ignores the fact that several translation directions might have similar data sizes and thus have similar bad performance, so instead of only focusing on the one worse-performing direction, both <a href="https://arxiv.org/abs/1909.02060">Oren et al., 2019</a> and <a href="https://arxiv.org/abs/2109.04020">Zhou et al., 2021</a> proposes to minimize the loss of a <strong>set</strong> of worse performing domains/languages.</p>

<p>Specifically, <a href="https://arxiv.org/abs/1909.02060">Oren et al., 2019</a> minimize a fixed <strong>fraction</strong> of worse performing domains in general language modeling.</p>

<p><a href="https://arxiv.org/abs/2109.04020">Zhou et al., 2021</a> minimizes the worst-case weight average loss of all language pairs, where the weights/sampling ratios are close to proportional sampling. More intuitively, the author first finds some adversarial weights that are close to proportional weights but yields the worst possible loss of all the weights that are close and aims to minimize this loss. Again, you can be creative in how to define closeness for two probability distributions, but in their work, they used the \(\chi\)-Divergence because it has some nice properties [<a href="https://arxiv.org/abs/1610.02581">Duchi and Namkoong, 2016</a>; <a href="https://arxiv.org/abs/1806.08010">Hashimoto et al., 2018</a>].</p>

<p>Experiment results show that maximizing the worst-case loss can improve the performance of LRLs while minimally sacrificing the performance of HRLs, essentially pushing forward the Pareto frontier.</p>

<p><a href="https://openreview.net/forum?id=Rv3vp-JDUSJ">Li and Gong, 2021</a> find weights for each direction that guide the optimization process towards a flatter minimum. The improvements are more significant compared to our previously introduced Distributionally Robust Optimization works, which highlight that the optimization process for multilingual translation should take the differences in convergence (or, in this case, curvature) into account.</p>

<p>We can also make the weights <strong>learnable</strong>. Depending on how to measure how <strong>well</strong> is the training process going, we bias our sampling ratio towards training regimes that are <strong>well</strong>.</p>

<p>For example, we can select a sampling ratio so that our training gradient is most similar to the development gradient [<a href="https://arxiv.org/abs/2004.06748">Wang et al., 2020</a>].</p>

<p>We can also select a sampling ratio to minimize loss-related definitions of learning progress. [<a href="https://arxiv.org/abs/2110.06997">Kreutzer et al., 2021</a>].</p>

<p><u>But, perhaps the simplest of all methods is more robust and generalizable</u>:</p>

<p>Instead of searching for these fantastic weights, why don’t we train on HRLs and then train on a mixture of HRLs and LRLs? Well, it turns out this simple method works surprisingly well:</p>

<p><a href="https://openreview.net/forum?id=7RMGI4slcb">Choi et al., 2023</a> proposes to first train on HRLs, then “fine-tune” on a mixture of HRLs and LRLs, which is equivalent to tuning the temperature but in a more coarse-grained way. Instead of using the training signals (gradient, activations), this work manually divides the training into two stages - training on HRLs first and a mix of high and low resource languages second. This simple trick solves the mismatch in convergence that causes overfitting on the LRLs.</p>

<h2 id="gradient-projection">Gradient Projection</h2>

<p>It was not until recently [<a href="https://arxiv.org/abs/2209.11379">Xin et al., 2022</a>] that we realized we don’t need fancy techniques designed for multi-task learning to solve unbalanced training in multilingual translation. Simple scalarization often yields strong baselines that are tough to beat. However, there have been extensive studies on how to manipulate the gradients in general multi-task learning setups, and people have been trying them on multilingual machine translation.</p>

<p>It is natural to assume that the gradient conflict between different tasks causes interference. Therefore, prior research has developed methods to either drop some of the conflicting gradients [<a href="https://arxiv.org/abs/2010.06808">Chen et al., 2020</a>], project one gradient to the orthogonal plane of another [<a href="https://arxiv.org/abs/2001.06782">Yu et al., 2020</a>; <a href="https://arxiv.org/abs/2109.04778">Yang et al., 2021</a>], or taking language similarity into account and project one gradient to a plane where the cosine similarity between gradients reflect language similarity [<a href="https://arxiv.org/abs/2010.05874">Wang et al., 2020</a>]. See the following figure for an illustration of these methods:</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Grad-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Grad-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Grad-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Grad.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="564" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>Not surprizingly, <a href="https://arxiv.org/abs/2010.05874">Wang et al., 2020</a> finds that the gradient similarity positively correlates with language familiies, i.e. languages from the same family are likely to have similar gradients. I reproduced some of their results in a English-to-many multilingual translation setting and found that the gradient similarity seems to have more to do with language order than script:</p>

<table>
  <thead>
    <tr>
      <th>Similar script, Same order</th>
      <th>Gradient Similarity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>French, Portugese</td>
      <td>0.35</td>
    </tr>
    <tr>
      <td>Different script, same order</td>
      <td> </td>
    </tr>
    <tr>
      <td>French, Russian</td>
      <td>0.13</td>
    </tr>
    <tr>
      <td>French, Korean</td>
      <td>0.22</td>
    </tr>
    <tr>
      <td>Similar script, different order</td>
      <td> </td>
    </tr>
    <tr>
      <td>Chinese, Japanese</td>
      <td>0.06</td>
    </tr>
    <tr>
      <td>French, Turkish</td>
      <td>0.12</td>
    </tr>
    <tr>
      <td>Different script, different order</td>
      <td> </td>
    </tr>
    <tr>
      <td>French, Japanese</td>
      <td>0.06</td>
    </tr>
    <tr>
      <td>Chinese, Korean</td>
      <td>0.008</td>
    </tr>
  </tbody>
</table>

<p>As promising as gradient projection methods might seem, there has been compelling evidence recently that gradient deconfliction does not outperform simple static scalarization in the general multi-task learning setting [<a href="https://arxiv.org/abs/2209.11379">Xin et al., 2022</a>; <a href="https://arxiv.org/abs/2201.04122">Kurin et al., 2022</a>] and specifically for machine translation [<a href="https://arxiv.org/abs/2304.03216">Chen et al., 2023</a>].</p>

<h2 id="discussion-and-future-work">Discussion and Future Work</h2>

<p>As you can see, a lot of the papers introduced here are from the past one or two years, so the whole area of how to elegantly handle the data size mismatch in multilingual translation is still a trendy topic.</p>

<p>We also see that <strong>scalarization</strong> and <strong>gradient projection</strong> modify two different parts of the gradient: scalarization mostly operates on the magnitude, and projection mainly operates on the direction (but magnitude is also involved).</p>

<h4 id="future-directions">Future Directions</h4>

<p>Here are some of my thoughts on future directions here: all are challenging but exciting to pursue, and I envision myself working in these directions.</p>

<ul>
  <li>
    <p>Find better ways to do dynamic scalarization without additional computational overhead - also needs to have strong improvements over <a href="https://openreview.net/forum?id=7RMGI4slcb">Choi et al., 2023</a>.</p>
  </li>
  <li>
    <p>Understand the optimization landscape of multilingual translation - and find which findings generalize to other deep multi-task settings (e.g., instruction tuning, LM pre-training) and also strong generalizability (e.g., zero-shot translation). One prevailing understanding is that many-to-one multilingual MT behaves more like general multi-task learning while yielding large improvements in the one-to-many setting is hard.</p>
  </li>
  <li>
    <p>Scale up! Language Model pre-training needs to find weights for each domain. <a href="https://arxiv.org/abs/2305.10403">Anil et al., 2022</a> did a grid search on the weights, which is expensive. There are many open questions: should these weights be static or dynamic? what is the most important factor when finding these weights [<a href="https://arxiv.org/abs/2302.03169">Xie et al., 2023a</a>, <a href="https://arxiv.org/abs/2305.10429">Xie et al., 2023b</a>] - as simple as size? what about quality? How do we find scalable methods to measure pre-training data quality/diversity?</p>
  </li>
</ul>]]></content><author><name></name></author><category term="sample-posts" /><category term="machine-translation" /><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">On Different Perspectives of Measuring Data Utility</title><link href="https://tianjianl.github.io/blog/2023/dataset-pruning/" rel="alternate" type="text/html" title="On Different Perspectives of Measuring Data Utility" /><published>2023-07-31T15:12:00+00:00</published><updated>2023-07-31T15:12:00+00:00</updated><id>https://tianjianl.github.io/blog/2023/dataset-pruning</id><content type="html" xml:base="https://tianjianl.github.io/blog/2023/dataset-pruning/"><![CDATA[<h2 id="overview">Overview</h2>

<p>Modern Deep Neural Networks are trained on a massive amount of data. Numerous works have proposed methods to estimate the contribution of each single datapoint. There are various ways to do this: to begin with, we can use hardcoded heuristics. For example, using sentence length and word frequency to determine whether a particular sentence is hard for a language model to learn. This method might seem hacky, but even the latest research on machine translation/language generation designs hardcoded heuristics based on the relative position of the token [<a href="https://arxiv.org/abs/2103.11088">Liang et al., 2021</a>, <a href="https://arxiv.org/abs/2211.11297">Jia et al., 2023</a>]. These methods usually induce numerous extra hyper-parameters and thus require careful tuning for performance. More importantly, relying on a task-specific heuristic does not generalize well to new tasks without domain-specific knowledge. I will not review the details of these methods in the blog post and instead focus on ways that show promises of generalization beyond the tasks the authors experimented on.</p>

<p>There are also lines of work that are more theoretically grounded by approximating the influence on training and validation loss of each datapoint. These lines differ in their way of doing this approximation. In this blog post, I will go over each of these methods and discuss each method’s strengths and limitations. This blog post also covers my thoughts on connecting data utility evaluation with curriculum learning and studies on the loss landscape.</p>

<p>At last, I will cover some of my latest thoughts for future directions. Research on data quality estimation traces back to the mid-90s but is still rapidly evolving. It would be crucial to design more efficient methods when the model and data sizes are scaling up exponentially.</p>

<h2 id="dataset-pruning">Dataset Pruning:</h2>

<p>A canonical way of estimating the contribution of individual parameters is by how much the training loss is affected when the parameter is removed. Similarly, we can also measure how the change of the loss is affected when a single datapoint is removed in a single batch.</p>

<p><a href="https://arxiv.org/abs/2107.07075">Paul et al., 2021</a> approaches this problem by analyzing the change in loss when the update steps are continuous: The time derivative of the loss for a given training example is given by:</p>

\[\Delta_t((x, y), S) = -\frac{\textrm{d} \ell(f_t(x), y)}{\textrm{d} t} = \frac{\textrm{d} \ell(f_t(x), y)}{\textrm{d} w_t} \cdot\frac{\textrm{d} w_t}{d t} = \textrm{grad}\cdot\frac{\textrm{d}w_t}{\textrm{d}t}\]

<p>Where \(S\) is the mini-batch in SGD or the entire training set in GD.</p>

<p>Intuitively, the metric \(\Delta_t ((x, y), S)\) measures the change in training loss when using an infinitesimal learning rate - the gradient flow. Now we bound the difference in loss of any example with an infinitesimal learning rate.</p>

<p><strong>Theorem</strong>: Up to a constant \(c\), the change in loss for another example in the same batch under an infinitesimal learning rate setting when a specific datapoint in that batch is pruned is bounded by the gradient norm of that pruned example:</p>

<p>\(\|\Delta_t((x,y),S) - \Delta_t((x, y), S_{\neg j})\| \leq c\|g_t(x_j, y_j)\|\) </p>

<p>Please refer to the paper for detailed proof.</p>

<p>However, it is hard to calculate the gradient for a single example at training iteration \(t\): \(g_t(x_j, y_j)\) since we batch multiple training examples together and aggregate the gradients. If we assume that the gradients of each logit \(\nabla w_t f_t^{(k)}(x)\) are orthogonal and have similar sizes, we can approximate the per example gradients as:</p>

\[\|g_t(x_i, y_i)\|_2 = \|\nabla_{f^{(k)}} \ell(f_t(x), y)^\top \nabla_{w_t}f_t^{(k)}(x)\|_2\approx\|p(x)-y\|_2\]

<p>The R.H.S is the \(\ell_2\) norm of the error vector. The paper calibrates the error l2 norm (EL2N) with multiple training runs at a specific training iteration. The author finds out that the model can identify hard training examples/noise in data with a very high EL2N score, and pruning 40% to 50% percent of data in CIFAR10 and 20% to 30% of data in CIFAR100 matches the performance of full training.</p>

<p>EL2N scores (l2 norm of prediction vector - groundtruth one-hot vector) are highly effective in identifying data that are either too hard or contain noise (see the following picture from the paper for easy and hard examples in image classification).</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/high_el2n_pict-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/high_el2n_pict-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/high_el2n_pict-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/high_el2n_pict.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>However, one intriguing question is how we can know that the high error norm for a datapoint correctly identifies a certain datapoint as noise instead of the model being incompetent. Choosing the EL2N score of arbitrary checkpoints and testing out performance does not scale well to large datasets. Another question I had was how this method generalizes to tasks other than image classification. I tested out this dataset pruning method on a multilingual machine translation task by selecting four language pairs (en-{de, es, fr, it}) from the opus-100 dataset:</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/el2n_random_plots_averaged-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/el2n_random_plots_averaged-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/el2n_random_plots_averaged-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/el2n_random_plots_averaged.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>Although I only did one training run for each pruned sparsity, the difference between pruning high EL2N and random pruning is high, especially when more data is pruned, showcasing that this method generalizes well under a machine translation setup. But what is more interesting is that for machine translation, the maximal percentage of data you can prune without harming the performance is around 20% as opposed to the 30%-40% redundancy in image classification - which is counterintuitive because machine translation data, from my perspective, should be noisier than image classification data and thus a higher percentage of data can be pruned.</p>

<p>I want to wrap this section of this paper up by re-iterating the limitation of this method: calibration of the EL2N score requires several training runs. For classification tasks with only a few labels, this is not a problem as a single or a few runs might give you a reasonable estimate. However, in language modeling, where the label size is your dictionary’s size, the variance of EL2N is high. It thus requires trying out different iterations and multiple training runs for a more accurate estimate.</p>

<p><a href="https://arxiv.org/abs/2206.14486">Sorscher et al., 2022</a> presents a follow-up work by proposing to cluster the data using their representations in a pre-trained model (in the paper, they used the SWaV model on ImageNet) and find examples that are super far away from the clusters to detect data to prune. Directly applying this to language modeling or machine translation raises the problem of having a large number of clusters. In ImageNet, there is only 1000 classes resulting in 1000 cluster centers, whereas in language modeling, the vocabulary size is at least \(20 \times\) large.</p>

<p>However, suppose dataset pruning can beat scaling laws: we can train much better LMs without exponentially increasing the amount of data by applying unsupervised clustering methods on text. In that case, this research direction should be promising. The question remains how can we automatically filter high-quality data from the enormous amount of data we have and validate that the data we have is high quality without full training runs of large language models - is training a tiny LM on high-quality data sufficient for validating the quality of data?</p>

<h2 id="influence-functions">Influence Functions</h2>

<p>Another line of work [<a href="https://proceedings.mlr.press/v70/koh17a.html">Koh and Liang, 2017</a>, <a href="https://proceedings.neurips.cc/paper/2020/hash/e6385d39ec9394f2f3a354d9d2b88eec-Abstract.html">Pruthi et al., 2020</a>, <a href="https://arxiv.org/abs/2205.09329">Yang et al., 2023</a>] also estimates data utility by how much of a difference it makes when that specific datapoint is removed. However, instead of measuring the difference in <strong>loss</strong> as in the aforementioned works, they measure the difference in the <strong>parameters</strong>. Formally, if \(\hat{\theta}\) and \(\hat{\theta}_{\neg z}\) are the empirical risk minimizers of the training set with and without a certain datapoint \(z\), respectively, we measure the difference to represent the utility of the datapoint \(z\):</p>

\[\mathcal{I}_\theta(z) = |\hat{\theta} - \hat{\theta}_{\neg z}|\]

<p>Statistical theory <a href="https://conservancy.umn.edu/handle/11299/37076">[Cook and Weisberg, 1982]</a> gives us an approximation of change in parameters if a certain example is upweighted by a factor of \(\epsilon\) . This is equivalent to training on this augmented loss: \(\frac{1}{n} \sum_{i=1}^n \mathcal{L}(z_i, \theta) + \epsilon \mathcal{L}(z, \theta)\).</p>

\[\mathcal{I}_{\textrm{upweight}}(z) = \frac{\textrm{d} \hat{\theta}}{\textrm{d} \epsilon}|_{\epsilon=0} = -\mathcal{H}_{\hat{\theta}}^{-1}\nabla_\theta\ell( z, \theta ) \]

<p>Setting the upweighting factor to  \(\epsilon = -\frac{1}{n}\) is the same as removing the datapoint, therefore we approximate the utility of the datapoint by</p>

\[\mathcal{I}_\ell(z) = \left|\hat{\theta} - \hat{\theta}_{\neg z}\right| = \left|-\frac{1}{n}\mathcal{I}_{\textrm{upweight}}(z)\right|\]

<p>Ultimately we care about the test performance. Applying the chain rule, we can also approximate the change of the <strong>test</strong> loss when a particular training example \(z\) is removed:</p>

\[\mathcal{I}_{\textrm{upweight}}(z, z_\textrm{test}) = \frac{\textrm{d} \ell(z_\textrm{test}, \hat{\theta})}{\textrm{d} \hat{\theta}}\cdot \frac{\textrm{d} \hat{\theta}}{\textrm{d} \epsilon} =-\nabla_{\theta}\ell(z_\textrm{test}, \hat{\theta})^\top\mathcal{H}_{\hat{\theta}}^{-1}\nabla_{\theta}\ell(z, \hat{\theta})\]

<p>Here we are able to see that the difference in test loss is also upper bounded by the norm of the gradient on that particular training example scaled by constant factors. Moreover, it also shows whether pruning a particular example would improve or degrade test performance depends on <strong>the dot product</strong> <strong>between the training and test gradients</strong> - this is intuitive. Still, in practice, we cannot access the test set. Numerous works have used the dot product between the training gradient and development gradient as a proxy for data utility [<a href="https://arxiv.org/abs/1911.10088">Wang et al., 2020a</a>, <a href="https://arxiv.org/abs/2004.06748">Wang et al., 2020b</a>, <a href="https://arxiv.org/abs/2109.04778">Yang et al., 2021</a>]. Therefore, the influence function here takes three things into account for estimating data utility:</p>

<ul>
  <li>The similarity between training and test gradients \(\nabla_\theta\ell(z_\textrm{test}, \theta)^\top \nabla_\theta\ell(z, \hat{\theta})\);</li>
  <li>The local curvature of the loss function at the current training step \(\mathcal{H}^{-1}_{\hat{\theta}}\);</li>
  <li>The magnitude of training gradient: \(\|\nabla_\theta\ell(z, \hat{\theta})\|\).</li>
</ul>

<p>In theory, this should be a more accurate estimate, but this method is impractical as it requires us to compute the per-sample gradients, which is already expensive. Not to mention calculating the dot product and the Hessian during every training iteration.</p>

<p>Recent work reframes data selection as two discrete optimization problems [<a href="https://arxiv.org/abs/2205.09329">Yang et al., 2023</a>]:</p>

<ul>
  <li>Given a constraint on the change in the norm of parameters, find the largest subset of data that satisfies this constraint.</li>
  <li>Given the budget on the fraction of data to prune, find the subset of data that results in the minimal change in parameters.</li>
</ul>

<p>The essence of this method is that they consider the “group effect” when pruning data. One datapoint might have large gradient norms that seem unprunable, but when combined with another datapoint, they have a small gradient norm and can be pruned together. In the paper, the author solves the optimization problem with simulated annealing [<a href="https://link.springer.com/book/10.1007/978-94-015-7744-1">Van Laarhoven and Arts, 1987</a>]. A critical engineering trick to speed up estimation is only computing the influence on the last linear layer.</p>

<h2 id="connections-with-curriculum-learning">Connections with Curriculum Learning</h2>

<p>So far, we have assumed that the utility of a single datapoint is <strong>static</strong> and <strong>model agnostic</strong>. These methods pre-compute the utility of data, filter out the noisy ones, and launch another training run on the pruned dataset. But in fact, the estimations (EL2N or Influence Functions) take the models’ current states into account so they should be dynamic. A naive connection would apply this to curriculum learning, where each datapoint is assigned a score and presented to the model adhering to a given schedule.</p>

<p>The motivation is that humans learn knowledge at different paces and models as well [<a href="https://arxiv.org/abs/2012.03107">Wu et al., 2021</a>], so we should present the easier data first, then more complex data, or general domain data first, then in-domain data. Existing research of curriculum learning for machine translation/language modeling ranks the training data with pre-defined heuristics. e.g., the perplexity of the text when evaluated by a trained language model [<a href="https://aclanthology.org/P10-2041/">Moore and Lewis, 2010</a>], sentence length and averaged word frequency [<a href="https://aclanthology.org/N19-1119/">Platanios et al., 2019</a>], similarity to the in-domain data [<a href="https://aclanthology.org/N19-1189/">Zhang et al., 2019</a>]. On a token level, this can be the position of the token [<a href="https://aclanthology.org/2021.findings-emnlp.310/">Liang et al., 2022</a>, <a href="https://aclanthology.org/2023.acl-long.666/">Jia et al., 2023</a>].</p>

<p>However, the schedule of how we should present the data stream with different scores is not well studied. The above work usually handcrafts several schedules and tests them out empirically, inducing many additional hyper-parameters to test out. Suppose we apply the pruning metrics which considers the model’s current state, we can design automated curriculums without pre-defined metrics or requiring a handcrafted schedule.</p>

<h2 id="future-directions">Future Directions</h2>

<p>Moving forward, I think there are some promising research directions that are worth exploring:</p>

<ul>
  <li><strong>Dataset Pruning on large scale text corpora</strong>: So far, all of these dataset pruning papers experiment on the task of image classification data, but does the findings still hold when evaluated on datasets that are magnitudes larger? How can we adapt these methods to language modeling/machine translation if not?</li>
  <li><strong>The theory behind curriculum learning</strong>: Why does presenting the data in a given order speeds up training or have better generalizability? Does it guide the model to a smoother area in the loss landscape?</li>
  <li><strong>Evaluating models beyond test loss</strong>: Does the distribution of data quality estimated by the model also give an estimate of the model itself?</li>
</ul>]]></content><author><name></name></author><category term="sample-posts" /><category term="dataset-pruning" /><summary type="html"><![CDATA[Overview]]></summary></entry></feed>