<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>In Defense of Vanilla DPO for Language Model Alignment | Tianjian  Li</title>
    <meta name="author" content="Tianjian  Li">
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">
    <meta name="keywords" content="Natural Language Processing, Machine Translation, Multilinguality">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://tianjianl.github.io/blog/2024/dpo/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tianjian </span>Li</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/assets/pdf/CV.pdf">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">In Defense of Vanilla DPO for Language Model Alignment</h1>
    <p class="post-meta">December 6, 2024</p>
    <p class="post-tags">
      <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>
        ·  
        <a href="/blog/tag/alignment">
          <i class="fas fa-hashtag fa-sm"></i> alignment</a>  
          
        ·  
        <a href="/blog/category/sample-posts">
          <i class="fas fa-tag fa-sm"></i> sample-posts</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <p>Direct Preference Optimization (DPO; <a href="https://arxiv.org/abs/2305.18290" rel="external nofollow noopener" target="_blank">Rafailov et al., 2023</a>) has emerged as the de-facto alignment algorithm for language models. It has been employed by many industrial level language models, which includes but is not limited to Meta’s Llama 3 (<a href="https://arxiv.org/abs/2407.21783" rel="external nofollow noopener" target="_blank">Dubey et al., 2024</a>), AI2’s Tulu (<a href="https://arxiv.org/abs/2411.1512" rel="external nofollow noopener" target="_blank">Lambert et al., 2024</a>), Alibaba’s Qwen (<a href="https://arxiv.org/abs/2407.10671" rel="external nofollow noopener" target="_blank">Yang et al., 2024</a>). The intuition is that DPO aims to <strong>directly</strong> increase the log-likelihood of human-preferred responses while decreasing the log-likelihood of dispreferred ones. Compared to standard RLHF (<a href="https://arxiv.org/abs/2009.01325" rel="external nofollow noopener" target="_blank">Stiennon et al., 2020</a>), the asymptotically equivalent DPO shines due to its simplicity - It requires neither on-policy sampling nor training a separate reward model.</p>

<p>Despite its simplicity and effectiveness, DPO has faced many criticisms (<a href="https://arxiv.org/abs/2405.19534" rel="external nofollow noopener" target="_blank">Chen et al., 2024</a>, <a href="https://arxiv.org/abs/2402.13228" rel="external nofollow noopener" target="_blank">Pal et al., 2024</a>,<a href="https://arxiv.org/abs/2404.12358" rel="external nofollow noopener" target="_blank">Rafailov et al., 2024</a>,<a href="https://arxiv.org/abs/2410.08847v2" rel="external nofollow noopener" target="_blank">Razin et al., 2024</a>, <a href="https://arxiv.org/abs/2405.14734" rel="external nofollow noopener" target="_blank">Meng et al., 2024</a>, <a href="https://kyunghyuncho.me/a-proper-preference-optimization-loss-and-its-gradient/" rel="external nofollow noopener" target="_blank">Cho 2024</a>). We summarize the criticisms below:</p>

<ul>
  <li>
    <p><strong>Likelihood Displacement</strong>: The likelihood of the chosen and the rejected response simultaneously decreases. Existing work accounts this to either the edit distance between the chosen and the rejected is short (<a href="https://arxiv.org/abs/2402.13228" rel="external nofollow noopener" target="_blank">Pal et al., 2024</a>) or the embedding distance the chosen and rejected is small (<a href="https://arxiv.org/abs/2410.08847v2" rel="external nofollow noopener" target="_blank">Razin et al., 2024</a>). Too similar pairs make the model fail to differentiate between the chosen and the rejected, resulting in the model simutaneously reducing the likelihood of both.</p>
  </li>
  <li>
    <p><strong>Poor Ranking Accuracy</strong>: Even after DPO, the model fails to assign higher probability to chosen responses compared to rejected responses (<a href="https://arxiv.org/abs/2405.19534" rel="external nofollow noopener" target="_blank">Chen et al., 2024</a>). This is an expected outcome due to likelihood displacement.</p>
  </li>
  <li>
    <p><strong>Mismatch Between Data and Policy</strong>: During pure offline DPO, where the preference data are collected from various models, e.g., Ultrafeedback (<a href="https://arxiv.org/abs/2310.01377" rel="external nofollow noopener" target="_blank">Cui et al., 2024</a>), there exists a mismatch between what the model can generate and what is optimized. To mitigate this issue, there has been many works that propose on-policy DPO (<a href="https://arxiv.org/abs/2312.16682" rel="external nofollow noopener" target="_blank">Xu et al., 2024</a>, <a href="https://arxiv.org/abs/2309.00267v3" rel="external nofollow noopener" target="_blank">Guo et al., 2024</a>, <a href="https://arxiv.org/abs/2401.10020" rel="external nofollow noopener" target="_blank">Yuan et al., 2024</a>) that generates the pair of responses from the reference policy rather than collecting preferences offline. Another solution would be to train the model first on chosen responses (<a href="https://arxiv.org/abs/2405.14734" rel="external nofollow noopener" target="_blank">Meng et al., 2024</a>).</p>
  </li>
</ul>

<p>A natural question arises: <strong>despite all these criticisms, why is vanilla DPO still the go-to algorithm when aligning language models?</strong></p>

<p>In this blogpost, I aim to give a non-rigorous explanation of the following two research questions:</p>

<ul>
  <li>Why does the likelihood displacement phenomenon occur?</li>
  <li>Why DPO works despite likelihood displacement on off-policy data?</li>
</ul>

<p>By answering these questions, I aim to defend against vanilla DPO for alignment, and to shed light on a deeper understanding of what DPO is doing to our model.</p>
<h3 id="preliminaries">Preliminaries:</h3>

<p>For a given user prompt \(x\), the language model \(\pi(\cdot \mid x)\) yields a probablity distribution over all possible responses \(\mathbf{y}\).</p>

<p>The pipeline begins with training the base language model \(\pi_\text{base}\) on high-quality generations \(\mathcal{D}_\text{SFT} = \{(x, \mathbf{y}), ...\}\). The SFT loss is given by</p>

\[\mathcal{L}_\textrm{SFT} = - \log \pi(\mathbf{y} \mid x)\]

<p>This stage of the pipeline is called Supervised Fine-Tuning (SFT), which produces the SFT’ed model \(\pi_\textrm{SFT}\).</p>

<p>Then there is another dataset \(\mathcal{D}_\textrm{DPO} = \{(x, \mathbf{y}_w, \mathbf{y}_l) ... \}\), where there are two generations \((y_w, y_l)\) for a given prompt \(x\), where \(\mathbf{y}_w\) is preferred (usually referred to as the “winning” or “chosen” response), and \(\mathbf{y}_l\) is dispreferred (usually referred to as the “losing” or “rejected” response). 
The DPO loss is given by:</p>

\[\mathcal{L}_\textrm{DPO} = -\log \sigma\left(\beta\log \frac{\pi_\theta(\mathbf{y}_w \mid x)}{\pi_\text{SFT}(\mathbf{y}_w \mid x)} - \beta\log\frac{\pi_{\theta}(\mathbf{y}_l \mid x)}{\pi_\textrm{SFT}(\mathbf{y}_l \mid x)}\right),\]

<p>which aims to maximize the margin between the increase in log-probability of the chosen response \(\log \pi_\theta(\mathbf{y}_w \mid x) - \log \pi_\text{SFT}(\mathbf{y}_w \mid x)\), and the increase in log-probability of the rejected response \(\log \pi_\theta(\mathbf{y}_l \mid x) - \log \pi_\text{SFT}(\mathbf{y}_l \mid x)\). In an ideal setting, the DPO loss should push the log-likelihood of the chosen response higher and push the log-likelihood of the rejected response lower, making the preferred response more likely under the aligned model.</p>

<p>However, as many works have noticed (<a href="https://arxiv.org/abs/2404.12358" rel="external nofollow noopener" target="_blank">Rafailov et al., 2024</a>, <a href="https://arxiv.org/abs/2402.13228" rel="external nofollow noopener" target="_blank">Pal et al., 2024</a>, <a href="https://arxiv.org/abs/2410.08847v2" rel="external nofollow noopener" target="_blank">Razin et al., 2024</a>, <a href="https://arxiv.org/abs/2404.04626" rel="external nofollow noopener" target="_blank">Feng et al., 2024</a>), often times the <strong>likelihood of both the chosen and the rejected response goes down</strong>, a phenomenon that <a href="https://arxiv.org/abs/2410.08847v2" rel="external nofollow noopener" target="_blank">Razin et al., 2024</a> terms “likelihood displacement”. In this blogpost, I would like to offer another explanation of why does “likelihood displacement” happens and more interestingly, why DPO is a strong baselines despite “likelihood displacement”. To this, we first need to investigate the effect of DPO onto the language modeling distribution — “The Squeezing Effect”.</p>

<h2 id="the-squeezing-effect-of-dpo-negative-gradient">The Squeezing Effect of DPO Negative Gradient</h2>

<p>The DPO gradient consists of two parts, a positive gradient that increases the likelihood of the chosen response, and a negative gradient that decreases the likelihood of the rejected response. <a href="https://arxiv.org/abs/2404.04626" rel="external nofollow noopener" target="_blank">Feng et al., 2024</a> shows that the negative gradient trumps the positive one.</p>

<p>If we take one step further and dig into the negative gradient, <a href="https://arxiv.org/abs/2407.10490" rel="external nofollow noopener" target="_blank">Ren and Sutherland, 2024</a> introduced an interesting effect of the negative gradient in DPO: both the chosen and rejected response goes down, and as a result, <strong>the majority of the probability mass is carried to sequences with very high likelihood.</strong> Intuitively, the “rich gets richer”, i.e. sentences with already high probability gets even higher due to the DPO negative gradient.</p>

<p>In the paper, they wrote as:</p>

<p>”The decreased probability mass is largely “squeezed” into the output which was most confident before the update. That is, if \(y^* = \arg \max_{i \in [V]\backslash \{y_l\}} \pi_{\theta}^t(y = i)\) , then \(\pi_{y = y^*}\) is guaranteed to increase.“</p>

<p>The following figure from <a href="https://arxiv.org/abs/2407.10490" rel="external nofollow noopener" target="_blank">Ren and Sutherland, 2024</a> illustrates the squeezing effect of DPO. As the log-likelihood of the rejected sentence \(\pi(\mathbf{y}_{u}^-)\) gets pushed down, the majority of the increased mass goes to \(\mathbf{y^*}\), the most probable sentence, instead of the chosen (winning) sentence \(\mathbf{y}_u^{+}\).</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Squeezing-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Squeezing-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Squeezing-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Squeezing.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="400" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<p>To validate this claim, <a href="https://arxiv.org/abs/2407.10490" rel="external nofollow noopener" target="_blank">Ren and Sutherland, 2024</a> performed a series of experiments: they plot out the log-likelihood of the chosen sentence and its paraphrases, the rejected sentence and its paraphrases, and a proxy of \(\mathbf{y}^*\) - the greedy decoded sentence: the following figure from the paper illustrates this:</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/chosen_rejected_decrease-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/chosen_rejected_decrease-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/chosen_rejected_decrease-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/chosen_rejected_decrease.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<p>In the left 4 subfigures, we observe that the likelihood of the chosen and its paraphrases, the rejected and its paraphrases all get decreased during DPO. However, the greedy decoded sentence, which is the black one in the rightmost subfigure goes up, thus confirming that the most the the probability mass goes to \(\mathbf{y}^*\)(or at least a proxy of \(\mathbf{y}^*\)).</p>

<p>Therefore, we can give a explanation of why “likelihood displacement” happens: it is due to the DPO negative gradient squeezes the probability masses to \(\mathbf{y}^*\) , often leading to decrease of both  \(\pi(\mathbf{y}_w \mid x)\) and \(\pi(\mathbf{y}_l \mid x)\). Now we aim to answer the second question: why are the benefits of increasing \(\mathbf{y}^*\)?</p>
<h2 id="the-benefits-of-increasing-mathbfy">The Benefits of Increasing \(\mathbf{y}^*\)</h2>

<p>Now that we know the reason why DPO decreases both the chosen and the rejected response is because it carried most of the decreased probability mass to \(\mathbf{y}^*\), we want to know why DPO, even the naive offline version, is still able to yield strong baselines, and is often the go-to method for industry labs to build strong models.</p>

<p><strong>It turns out that \(\mathbf{y}^* = \arg\max_\mathbf{y} \pi(\mathbf{y} \mid x)\) itself is a very strong baseline.</strong> In <a href="https://arxiv.org/pdf/2412.01951" rel="external nofollow noopener" target="_blank">Huang et al., 2024</a>, the authors were interested in the mechanisms behind self-improvement - why can language models be improved when training on data that is generated by itself (<a href="https://aclanthology.org/P95-1026.pdf" rel="external nofollow noopener" target="_blank">Yarowsky, 1995</a>, <a href="https://arxiv.org/abs/1909.13788" rel="external nofollow noopener" target="_blank">He et al., 2020</a>, <a href="https://arxiv.org/abs/2203.14465" rel="external nofollow noopener" target="_blank">Zelikman et al., 2022</a>, <a href="https://arxiv.org/abs/2312.16682" rel="external nofollow noopener" target="_blank">Xu et al., 2023</a>), sometimes even without external feedback (<a href="https://arxiv.org/abs/2401.10020" rel="external nofollow noopener" target="_blank">Yuan et al., 2024</a>)?</p>

<p><a href="https://arxiv.org/abs/2412.01951" rel="external nofollow noopener" target="_blank">Huang et al., 2024</a> points out that the best-of-N response with the highest likelihood:
\(\mathbf{\hat{y}}^* = \arg\max_{\mathbf{y}_i,~i \in \{1,...,N\}} \pi(y_i\mid x)\)
is actually a strong baseline, and training a language model on high-likelihood responses can “sharpen” the model towards these high-likelihood responses. (It would be interesting to track the best-of-N log-likelihood throughout DPO).</p>

<p>Empirically, <a href="https://arxiv.org/abs/2412.01951" rel="external nofollow noopener" target="_blank">Huang et al., 2024</a> validated their results on a large suite of models (Phi, Mistral, Llama, GPT-3.5) across various tasks (MATH, GSM, …). The results are in the following figure:</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Experiments-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Experiments-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Experiments-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Experiments.png" class="img-fluid rounded z-depth-1 mx-auto d-block" width="600" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<p>In subfigure (a), the numbers refer to the percent accuracy improvement over greedy decoding. The authors find that:</p>
<ol>
  <li>Best-of-N (with highest log-likelihood) is always better than naive sampling with temperature = 1.0</li>
  <li>For all datasets, Best-of-N improves upon greedy decoding, for at least one model.</li>
  <li>For every model, there is at least one dataset where Best-of-N improves upon greedy decoding.</li>
</ol>

<p>In the middle subfigure (b), x-axis is the N in best-of-N and the y-axis is the percentage improvement over greedy decoding - this shows that finding sequences with high likelihood is able to yield a much stronger baseline than greedy decoding (which is already a strong baseline).</p>

<p>In the right subfigure (c), the histograms of correct and incorrect responses are plotted, we can see that the correct response tends to have higher likelihood under the base model.</p>

<p>These findings indicate that the “hidden knowledge” for self-improvement is hidden at the probabilities -  sequences with high log-probability is more likely to be correct. \(\mathbf{y}^*\) , or the most probable sentence is most likely to be correct.</p>

<p>So far, we have learned that:</p>

<ul>
  <li>
    <p>DPO pushes up sentences with <strong>super high log-probability</strong> under the model, especially when the winning and losing response are offline samples (<a href="https://arxiv.org/abs/2407.10490" rel="external nofollow noopener" target="_blank">Ren and Sutherland, 2024</a>).</p>
  </li>
  <li>
    <p>Sentences with <strong>super high log-probability</strong> are more likely to be correct answers / better responses (<a href="https://arxiv.org/abs/2412.01951" rel="external nofollow noopener" target="_blank">Huang et al., 2024</a>).</p>
  </li>
</ul>

<p>Now, It should be intuitive that why DPO (even the offline variant) is able to yield a strong baseline - it “squeezes” or “sharpens” up the log-likelihood of sentences with already large log-probability, regardless of the winning and losing response construction (because it is very unlikely that the winning and / or the losing response is \(\mathbf{y}^*\)). There is also empirical evidence that performing vanilla DPO on “wrong-over-wrong” responses (<a href="https://arxiv.org/abs/2410.11055" rel="external nofollow noopener" target="_blank">Yao et al., 2024</a>) can improve performance.</p>

<h2 id="limitations">Limitations</h2>

<p>Admittedly, this explanation of DPO moves masses to \(\mathbf{y}^*\) is a simplified version of what is happening in practice, otherwise we would be living in a world where we simple impose the negative gradient on every offline data to sharpen the model towards high log-prob sentences. But I think this explanation is interesting and motivates to think of more reliable reward signals beyond model log-probs.</p>


    </div>
  </article>
</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Tianjian  Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
